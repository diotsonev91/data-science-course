{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f89d9179-fa6f-4090-bc95-2a38f4dede93",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This project focuses on the task of classifying images of fruits using Convolutional Neural Networks (CNNs). The primary goal is to explore how image classification can be effectively achieved under **constrained conditions**, such as **limited data availability** and **resource-limited deployment environments** (e.g., embedded systems or smartphones).\n",
    "\n",
    "Image classification is widely used in applications like automated food sorting, agricultural monitoring, quality control, and retail automation. Efficient and accurate classification systems can significantly reduce manual labor, increase throughput, and ensure consistency across vision-based workflows.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this study, we aim to:\n",
    "\n",
    "- Build and train a **lightweight CNN** capable of classifying fruit images using **grayscale input**.\n",
    "- Apply **lightweight grayscale-specific augmentations** (e.g., flips, small rotations, noise) only to training data, to improve robustness without violating low-data constraints.\n",
    "- Optimize for **small model size and fast inference**, making the model suitable for edge deployment.\n",
    "- Compare the performance of our lightweight CNN with a **well-known pretrained model (MobileNetV2)** to understand trade-offs in accuracy, size, and complexity.\n",
    "\n",
    "## Key Features of This Project\n",
    "\n",
    "- ### Data-Efficient Design  \n",
    "  We work with a **small dataset** and **grayscale images**, using **minimal augmentations** to improve generalization while simulating real-world data scarcity.\n",
    "\n",
    "- ### Deployment-Ready Modeling  \n",
    "  The CNN architecture is selected and trained to support **low-power, real-time inference** on embedded hardware or mobile devices. Post-training, we apply **quantization** (float32 → int8) to reduce memory and latency.\n",
    "\n",
    "- ### Comparative Evaluation  \n",
    "  We benchmark our model against a **pretrained MobileNetV2**, comparing trade-offs between size, accuracy, and complexity in a deployment context.\n",
    "\n",
    "- ### Transparent Experimentation  \n",
    "  All preprocessing, modeling, and evaluation steps are clearly explained and reproducible, with detailed tracking of accuracy and inference performance.\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "By exploring how far a compact CNN can go under tight constraints, this project contributes to the field of **edge-ready deep learning** — enabling smart, efficient vision systems even where compute and data are scarce.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1156c5aa-7138-4462-a126-309110001b7a",
   "metadata": {},
   "source": [
    "# Step 1: Data Analysis & Dataset Sample\n",
    "\n",
    "## 1.1 Data Acquisition\n",
    "\n",
    "The dataset used in this project is the [Fruits 360 dataset](https://www.kaggle.com/datasets/moltean/fruits), publicly available on Kaggle. It contains thousands of labeled fruit images in `.jpg` format, sorted into folders by fruit class. Each image is **100×100 pixels** and originally in **RGB format**.\n",
    "\n",
    "We selected this dataset because:\n",
    "- It includes a **wide variety of fruit classes** suitable for multi-class classification.\n",
    "- Images are already **clean and labeled**, minimizing preprocessing effort.\n",
    "- It is a well-known benchmark dataset for fruit classification tasks.\n",
    "- The folder structure is compatible with **PyTorch’s `ImageFolder`**, making it easy to load.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 Dataset Reduction & Cleaning\n",
    "\n",
    "To simulate a more realistic and constrained edge-case scenario, we **manually selected 8 specific fruit categories** from the dataset. This allows us to:\n",
    "- Focus the classification task on a smaller, balanced subset.\n",
    "- Reduce dataset size for faster training and evaluation.\n",
    "- Better explore model performance in a **low-data regime**.\n",
    "\n",
    "We ensured that:\n",
    "- The image dimensions are consistent (**100×100**).\n",
    "- All images are intact and correctly labeled.\n",
    "- Class balance is maintained across training and test splits.\n",
    "\n",
    "Additionally, during preprocessing, all images are converted to **grayscale** to reduce input complexity and align with embedded system constraints.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.3 Exploratory Data Analysis (EDA)\n",
    "\n",
    "To confirm that our selected subset is suitable for fair training, we examined the **class distribution** using simple utility functions.\n",
    "\n",
    "### Count Function:\n",
    "```python\n",
    "def count_images_per_class(directory):\n",
    "    return {\n",
    "        class_name: len(os.listdir(os.path.join(directory, class_name)))\n",
    "        for class_name in os.listdir(directory)\n",
    "        if os.path.isdir(os.path.join(directory, class_name))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7755d773-2c0d-470c-b492-ebccacf8c4c8",
   "metadata": {},
   "source": [
    "\n",
    "- <a href=\"01_eda.ipynb\" target=\"_blank\">01_eda.ipynb</a> — Exploratory Data Analysis & Dataset Sampling  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9194545-57b4-4e4a-9a57-5900e7df0687",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Notebook Workflow\n",
    "- <a href=\"01_eda.ipynb\" target=\"_blank\">01_eda.ipynb</a> — Exploratory Data Analysis & Dataset Sampling  \n",
    "- <a href=\"02_preprocessing.ipynb\" target=\"_blank\">02_preprocessing.ipynb</a> — Image preprocessing, dataset loading  \n",
    "- <a href=\"03_train_cnn.ipynb\" target=\"_blank\">03_train_cnn.ipynb</a> — Baseline CNN training  \n",
    "- <a href=\"04_eval_mobilenetv2.ipynb\" target=\"_blank\">04_eval_mobilenetv2.ipynb</a> — MobileNetV2 evaluation  \n",
    "- <a href=\"05_results.ipynb\" target=\"_blank\">05_results.ipynb</a> — Final metrics, comparison, discussion  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
